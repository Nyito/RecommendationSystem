{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ricar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ricar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\ricar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7136 entries, 0 to 10001\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Product Name             7136 non-null   object \n",
      " 1   Category                 7136 non-null   object \n",
      " 2   Selling Price($)         7136 non-null   float64\n",
      " 3   About Product            7136 non-null   object \n",
      " 4   Product Specification    7136 non-null   object \n",
      " 5   Shipping Weight(Pounds)  7136 non-null   float64\n",
      " 6   Main Category            7136 non-null   object \n",
      " 7   Sub-Category             7136 non-null   object \n",
      " 8   Side Category            6155 non-null   object \n",
      "dtypes: float64(2), object(7)\n",
      "memory usage: 557.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') \n",
    "\n",
    "dataset = pd.read_csv(\"../data/AmazonData.csv\")\n",
    "\n",
    "# Excluding columns with null and not usefull values\n",
    "cols = [0,2,3,5,6,8,9,12,14,15,16,17,18,19,20,21,22,23,24,25,26,27]\n",
    "dataset.drop(dataset.columns[cols], axis =1, inplace=True)\n",
    "dataset.dropna(inplace = True)\n",
    "\n",
    "\n",
    "# Splitting Category in 3 parts\n",
    "new = dataset[\"Category\"].str.split(\"|\", n = 2, expand = True)\n",
    "  \n",
    "# making the first category called Main Category\n",
    "dataset[\"Main Category\"]= new[0] \n",
    "  \n",
    "# making the second category called sub_category \n",
    "dataset[\"Sub-Category\"]= new[1]\n",
    "\n",
    "# making the third category called side_category \n",
    "dataset[\"Side Category\"]= new[2]\n",
    "\n",
    "\n",
    "# Database Price and weight treatment\n",
    "dataset.rename(columns = {'Uniq Id':'Id','Shipping Weight':'Shipping Weight(Pounds)', 'Selling Price':'Selling Price($)'}, inplace = True)\n",
    "\n",
    "# Removing units from Price and Weight\n",
    "dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].str.strip('ounces')\n",
    "dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].str.strip('pounds')\n",
    "dataset['Selling Price($)'] = dataset['Selling Price($)'].str.replace('$', '')\n",
    "\n",
    "# Setting Column Selling Price as float value\n",
    "indexes = dataset[dataset['Selling Price($)'] == 'Total price:'].index\n",
    "dataset.drop(indexes, inplace=True)\n",
    "\n",
    "#\n",
    "dataset['Selling Price($)'] = dataset['Selling Price($)'].str.replace(',', '', regex=False)\n",
    "indexes = dataset[dataset['Selling Price($)'].str.contains('-', na=False)].index\n",
    "dataset.drop(indexes, inplace=True)\n",
    "\n",
    "#\n",
    "indexes = dataset[dataset['Selling Price($)'].str.contains('&', na=False)].index\n",
    "dataset.drop(indexes, inplace=True)\n",
    "\n",
    "#\n",
    "indexes = dataset[dataset['Selling Price($)'].str.contains('Currently', na=False)].index\n",
    "dataset.drop(indexes, inplace=True)\n",
    "\n",
    "#\n",
    "indexes = dataset[dataset['Selling Price($)'].str.contains('from', na=False)].index\n",
    "dataset.drop(indexes, inplace=True)\n",
    "\n",
    "#\n",
    "dataset['Selling Price($)'] = dataset['Selling Price($)'].str.split(' ').str[0]\n",
    "dataset['Selling Price($)'] = dataset['Selling Price($)'].astype(float)\n",
    "\n",
    "# Setting Column Shipping Weight as float value\n",
    "indexes = dataset[dataset['Shipping Weight(Pounds)'].str.contains(r'\\. ', na=False)].index\n",
    "\n",
    "dataset.at[1619, 'Shipping Weight(Pounds)']\n",
    "dataset.drop(1619, inplace=True)\n",
    "dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].str.replace(',', '', regex=False)\n",
    "dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].astype(float)\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessamento(texto):\n",
    "    texto = texto.replace('[^a-zA-Z]',' ').lower()\n",
    "    stop_re = '\\\\b'+'\\\\b|\\\\b'.join(nltk.corpus.stopwords.words('english'))+'\\\\b'\n",
    "    texto = texto.replace(stop_re, '')\n",
    "    texto = texto.split()\n",
    "    return texto\n",
    "\n",
    "# Function for preprocessing with stemming\n",
    "def preprocess_stemming(texto):\n",
    "    texto = texto.replace('[^a-zA-Z]', ' ').lower()\n",
    "    stop_re = '\\\\b' + '\\\\b|\\\\b'.join(nltk.corpus.stopwords.words('english')) + '\\\\b'\n",
    "    texto = texto.replace(stop_re, '')\n",
    "    texto = texto.split()\n",
    "\n",
    "    # Add stemming using PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_text = [stemmer.stem(word) for word in texto]\n",
    "\n",
    "    return stemmed_text\n",
    "\n",
    "# Function for preprocessing with lemmatization\n",
    "def preprocess_lemmatization(texto):\n",
    "    texto = texto.replace('[^a-zA-Z]', ' ').lower()\n",
    "    stop_re = '\\\\b' + '\\\\b|\\\\b'.join(nltk.corpus.stopwords.words('english')) + '\\\\b'\n",
    "    texto = texto.replace(stop_re, '')\n",
    "    texto = texto.split()\n",
    "\n",
    "    # Add lemmatization using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in texto]\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [db, longboards, coreflex, crossbow, 41\", bamb...\n",
       "1        [electronic, snap, circuits, mini, kits, class...\n",
       "2        [3doodler, create, flexy, 3d, printing, filame...\n",
       "3        [guillow, airplane, design, studio, with, trav...\n",
       "4                   [woodstock-, collage, 500, pc, puzzle]\n",
       "                               ...                        \n",
       "9995     [cozy, line, home, fashions, size, 2, piece, o...\n",
       "9996           [lego, 8-brick, storage, box,, bright, red]\n",
       "9998     [trends, international, nfl, la, chargers, hg,...\n",
       "9999     [newpath, learning, 10, piece, science, owls, ...\n",
       "10001    [hasegawa, ladders, lucano, step, ladder,, ora...\n",
       "Name: Processed Product Name, Length: 7136, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original text without preprocessing\n",
    "dataset[\"Processed Product Name\"] = dataset[\"Product Name\"].apply(preprocessamento)\n",
    "dataset[\"Processed Product Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [db, longboard, coreflex, crossbow, 41\", bambo...\n",
       "1        [electron, snap, circuit, mini, kit, classpack...\n",
       "2        [3doodler, creat, flexi, 3d, print, filament, ...\n",
       "3        [guillow, airplan, design, studio, with, trave...\n",
       "4                     [woodstock-, collag, 500, pc, puzzl]\n",
       "                               ...                        \n",
       "9995     [cozi, line, home, fashion, size, 2, piec, oce...\n",
       "9996            [lego, 8-brick, storag, box,, bright, red]\n",
       "9998     [trend, intern, nfl, la, charger, hg, -, mobil...\n",
       "9999     [newpath, learn, 10, piec, scienc, owl, and, o...\n",
       "10001     [hasegawa, ladder, lucano, step, ladder,, orang]\n",
       "Name: Processed (Stemming), Length: 7136, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess with stemming\n",
    "dataset['Processed (Stemming)'] = dataset[\"Product Name\"].apply(preprocess_stemming)\n",
    "dataset['Processed (Stemming)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [db, longboards, coreflex, crossbow, 41\", bamb...\n",
       "1        [electronic, snap, circuit, mini, kit, classpa...\n",
       "2        [3doodler, create, flexy, 3d, printing, filame...\n",
       "3        [guillow, airplane, design, studio, with, trav...\n",
       "4                   [woodstock-, collage, 500, pc, puzzle]\n",
       "                               ...                        \n",
       "9995     [cozy, line, home, fashion, size, 2, piece, oc...\n",
       "9996           [lego, 8-brick, storage, box,, bright, red]\n",
       "9998     [trend, international, nfl, la, charger, hg, -...\n",
       "9999     [newpath, learning, 10, piece, science, owl, a...\n",
       "10001    [hasegawa, ladder, lucano, step, ladder,, orange]\n",
       "Name: Processed (Lemmatization), Length: 7136, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess with lemmatization\n",
    "dataset['Processed (Lemmatization)'] = dataset[\"Product Name\"].apply(preprocess_lemmatization)\n",
    "dataset['Processed (Lemmatization)']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
