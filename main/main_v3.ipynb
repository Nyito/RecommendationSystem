{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ricar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ricar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\ricar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Developed algorithm libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rapidfuzz import fuzz, process, utils\n",
    "\n",
    "\n",
    "## References libraries\n",
    "# import turicreate as tc\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_database():\n",
    "    dataset = pd.read_csv(\"../data/AmazonData.csv\")\n",
    "    return dataset\n",
    "\n",
    "# Function to process the dataset\n",
    "def data_manipulation(dataset):\n",
    "    # Excluding columns that we dont use\n",
    "    cols = [0,2,3,5,6,8,9,12,14,15,16,17,18,19,20,21,22,23,24,25,26,27]\n",
    "    dataset.drop(dataset.columns[cols], axis =1, inplace=True)\n",
    "    dataset.dropna(inplace = True)\n",
    "\n",
    "    # Splitting Category in 3 parts\n",
    "    new = dataset[\"Category\"].str.split(\"|\", n = 3, expand = True)\n",
    "    \n",
    "    # making the first category called Main Category\n",
    "    dataset[\"Main Category\"]= new[0] \n",
    "    \n",
    "    # making the second category called sub_category \n",
    "    dataset[\"Sub Category\"]= new[1]\n",
    "\n",
    "    # making the third category called side_category \n",
    "    dataset[\"Side Category\"]= new[2]\n",
    "\n",
    "    # making the last column consist of the remaining categories\n",
    "    dataset[\"Other Category\"]= new[3]\n",
    "\n",
    "    # Dropping old category columns and the remaining categories \n",
    "    dataset.drop(columns =[\"Category\"], inplace = True)\n",
    "\n",
    "    # Setting Column Selling Price as float value\n",
    "    # Database Price and weight treatment\n",
    "    dataset.rename(columns = {'Uniq Id':'Id','Shipping Weight':'Shipping Weight(Pounds)', 'Selling Price':'Selling Price($)'}, inplace = True)\n",
    "\n",
    "    # Removing units from Price and Weight\n",
    "    dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].str.strip('ounces')\n",
    "    dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].str.strip('pounds')\n",
    "    dataset['Selling Price($)'] = dataset['Selling Price($)'].str.replace('$', '')\n",
    "\n",
    "    # Removing rows with Total Price invalid\n",
    "    indexes = dataset[dataset['Selling Price($)'] == 'Total price:'].index\n",
    "    dataset.drop(indexes, inplace=True)\n",
    "\n",
    "    # Removing rows with '-' character\n",
    "    dataset['Selling Price($)'] = dataset['Selling Price($)'].str.replace(',', '', regex=False)\n",
    "    indexes = dataset[dataset['Selling Price($)'].str.contains('-', na=False)].index\n",
    "    dataset.drop(indexes, inplace=True)\n",
    "\n",
    "    # Removing rows with '&' character\n",
    "    indexes = dataset[dataset['Selling Price($)'].str.contains('&', na=False)].index\n",
    "    dataset.drop(indexes, inplace=True)\n",
    "\n",
    "    # Removing rows with 'Currently' character\n",
    "    indexes = dataset[dataset['Selling Price($)'].str.contains('Currently', na=False)].index\n",
    "    dataset.drop(indexes, inplace=True)\n",
    "\n",
    "    # Removing rows with 'from' character\n",
    "    indexes = dataset[dataset['Selling Price($)'].str.contains('from', na=False)].index\n",
    "    dataset.drop(indexes, inplace=True)\n",
    "\n",
    "    # Adjusting values with wrong format\n",
    "    dataset['Selling Price($)'] = dataset['Selling Price($)'].str.split(' ').str[0]\n",
    "    dataset['Selling Price($)'] = dataset['Selling Price($)'].astype(float)\n",
    "\n",
    "    # Setting Column Shipping Weight as float value\n",
    "    indexes = dataset[dataset['Shipping Weight(Pounds)'].str.contains(r'\\. ', na=False)].index\n",
    "\n",
    "    dataset.at[1619, 'Shipping Weight(Pounds)']\n",
    "    dataset.drop(1619, inplace=True)\n",
    "    dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].str.replace(',', '', regex=False)\n",
    "    dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].astype(float)\n",
    "\n",
    "    return dataset\n",
    "def save_data_manipulation(dataset):\n",
    "    dataset.to_csv('../data/CleanData.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisito #02\n",
    "def load_data():\n",
    "    dataset = load_database()\n",
    "    dataset = data_manipulation(dataset)\n",
    "    return dataset\n",
    "\n",
    "# Data split to sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "def split_data_sets(dataset):\n",
    "    train, test = train_test_split(dataset, test_size=0.2)\n",
    "    return train, test\n",
    "\n",
    "# Save data (optional)\n",
    "def save_data_sets(train, test):\n",
    "    train.to_csv(\"../data/train.csv\", index=False)\n",
    "    test.to_csv(\"../data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7136 entries, 0 to 10001\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Product Name             7136 non-null   object \n",
      " 1   Selling Price($)         7136 non-null   float64\n",
      " 2   About Product            7136 non-null   object \n",
      " 3   Product Specification    7136 non-null   object \n",
      " 4   Shipping Weight(Pounds)  7136 non-null   float64\n",
      " 5   Main Category            7136 non-null   object \n",
      " 6   Sub Category             7136 non-null   object \n",
      " 7   Side Category            6155 non-null   object \n",
      " 8   Other Category           2724 non-null   object \n",
      "dtypes: float64(2), object(7)\n",
      "memory usage: 557.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data()\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisitos #03 (Our Recommendation Algorithm )\n",
    "\n",
    "# Load data for training model\n",
    "def load_data_parameters():\n",
    "    # Aqui seria possível implementar a leitura dos parâmetros para o modelo\n",
    "    return {'vector_size': 100, 'window': 5, 'min_count': 1,'workers': 4}\n",
    "\n",
    "# Processing text function\n",
    "def preprocess_text(text):\n",
    "    text = text.replace('[^a-zA-Z]',' ').lower()\n",
    "    stop_re = '\\\\b'+'\\\\b|\\\\b'.join(nltk.corpus.stopwords.words('english'))+'\\\\b'\n",
    "    text = text.replace(stop_re, '')\n",
    "    text = text.split()\n",
    "\n",
    "    # Add lemmatization using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemmatized_text\n",
    "\n",
    "# Treinamento do modelo base com Word2Vec\n",
    "def train_word2vec_model(data_parameters, dataset):\n",
    "\n",
    "    model = Word2Vec(sentences=dataset[\"Processed Product Name\"], vector_size=data_parameters['vector_size'],window=data_parameters['window'], min_count=data_parameters['min_count'], workers=data_parameters['workers'])\n",
    "    return model\n",
    "\n",
    "# vectorizing text function\n",
    "def vectorize_product(product_name, model):\n",
    "    words = [word for word in product_name if word in model.wv]\n",
    "    if len(words) > 0:\n",
    "        return np.mean([model.wv[word] for word in words], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "    \n",
    "# Morfologic and Syntatic Filters\n",
    "def prototype_rapid_fuzz_filter(user_input, products, number_of_rec):\n",
    "    list_of_rec = []\n",
    "    token_set_ratio_match = process.extract(user_input, products, scorer=fuzz.token_set_ratio, limit=1, processor=utils.default_process)\n",
    "    partial_ratio_matches = process.extract(user_input, products, scorer=fuzz.token_set_ratio, limit=number_of_rec, processor=utils.default_process)\n",
    "\n",
    "    if token_set_ratio_match[0][1] == 100:\n",
    "        list_of_rec.extend(token_set_ratio_match)\n",
    "        for match in partial_ratio_matches:\n",
    "            if match[0] != token_set_ratio_match[0][0]:\n",
    "                list_of_rec.append(match)\n",
    "    \n",
    "    else:\n",
    "        for match in partial_ratio_matches:\n",
    "            list_of_rec.append(match)\n",
    "\n",
    "    return list_of_rec\n",
    "\n",
    "def rapid_fuzz_rec_to_df(recommendations, dataframe):\n",
    "    sorted_indeces = [match[2] for match in recommendations]\n",
    "    reordered_df = dataframe.iloc[sorted_indeces].reset_index(drop=True)\n",
    "    return reordered_df\n",
    "\n",
    "\n",
    "# Product Recommendation function\n",
    "def product_recommendation(product_vector, dataset, top_n=5):\n",
    "    # Calcular similaridades cosseno\n",
    "    similarities = dataset[\"Product Vector\"].apply(lambda x: cosine_similarity([product_vector], [x])[0][0])\n",
    "    \n",
    "    # Ordenar por similaridade e pegar os top_n produtos\n",
    "    top_indices = similarities.nlargest(top_n).index\n",
    "    \n",
    "    # Retornar o DataFrame com os produtos recomendados, mas mantendo os nomes originais\n",
    "    return dataset.loc[top_indices, dataset.columns != 'Product Vector']\n",
    "\n",
    "\n",
    "# Category Filter \n",
    "def category_filter(dataset, selected_product):\n",
    "\n",
    "    main_category_input = selected_product['Main Category']\n",
    "    sub_category_input = selected_product['Sub Category']\n",
    "    side_category_input = selected_product['Side Category']\n",
    "    other_category_input = selected_product['Other Category']\n",
    "\n",
    "    # Create a new column to calculate the score\n",
    "    dataset['score'] = 0\n",
    "\n",
    "    # Raises the score if categories match\n",
    "    dataset.loc[dataset['Main Category'] == main_category_input, 'score'] += 1\n",
    "    dataset.loc[dataset['Sub Category'] == sub_category_input, 'score'] += 1\n",
    "    dataset.loc[dataset['Side Category'] == side_category_input, 'score'] += 1\n",
    "    dataset.loc[dataset['Other Category'] == other_category_input, 'score'] += 1\n",
    "\n",
    "    # Sort the database based on the score\n",
    "    category_filter = dataset.sort_values(by='score', ascending=False)\n",
    "\n",
    "    max_score = category_filter['score'].max()\n",
    "\n",
    "    # Filter rows with the maximum score\n",
    "    category_filter = category_filter[category_filter['score'] == max_score]\n",
    "\n",
    "    # Removes the new column\n",
    "    category_filter = category_filter.drop(columns='score')\n",
    "\n",
    "    # return the sorted database\n",
    "    if category_filter.empty:\n",
    "        print('No recommendaation found for this product.')\n",
    "    else:\n",
    "        return category_filter\n",
    "\n",
    "def name_based_filter(dataset, product_name):\n",
    "\n",
    "    parameters = load_data_parameters()\n",
    "    # Applying text preprocess in dataset\n",
    "    dataset[\"Processed Product Name\"] = dataset[\"Product Name\"].apply(preprocess_text)\n",
    "\n",
    "    model = train_word2vec_model(parameters,dataset)\n",
    "\n",
    "    # Applying vectorizing function in dataset\n",
    "    dataset[\"Product Vector\"] = dataset[\"Processed Product Name\"].apply(lambda x: vectorize_product(x, model))\n",
    "\n",
    "    # Pré-processando o nome do produto fornecido pelo usuário\n",
    "    processed_product_name = preprocess_text(product_name)\n",
    "\n",
    "    # Vetorizando o nome do produto fornecido (passar o modelo aqui também)\n",
    "    product_vector = vectorize_product(processed_product_name, model)\n",
    "\n",
    "    recommendation = product_recommendation(product_vector, dataset, dataset.shape[0])\n",
    "    return recommendation\n",
    "\n",
    "def save_main_model(model):\n",
    "    # Create model pasta\n",
    "    model.save(\"../data/main_model.model\")\n",
    "\n",
    "def load_main_model():\n",
    "    # Create model pasta\n",
    "    return Word2Vec.load(\"../data/main_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select option:\n",
      "1- Select input from database\n",
      "2- Type product name\n",
      "Word2Vec: \n",
      "560     Heroes of Goo Jit Zu 41012 S1 Ultimate Hero Pk...\n",
      "61                  Domez My Hero Academia 4-Piece Bundle\n",
      "3668    Avengers Marvel Endgame Titan Hero Power Fx Ir...\n",
      "5408    Icon Heroes DC Heroes Black Adam 1: 9 Scale Po...\n",
      "5072                       Wildkin Heroes Rolling Luggage\n",
      "Name: Product Name, dtype: object\n",
      "\n",
      "\n",
      "RapidFuzz: \n",
      "('Domez My Hero Academia 4-Piece Bundle', 100.0, 61)\n",
      "('Banpresto 35783 My Hero Academia Enter The Hero Izuku Midoriya Figure', 100.0, 117)\n",
      "('Avengers Marvel Black Panther 6\"-Scale Marvel Super Hero Action Figure Toy', 100.0, 262)\n",
      "('Neon Super Hero Mask, Party Accessory', 100.0, 460)\n",
      "('Heroes of Goo Jit Zu 41012 S1 Ultimate Hero Pk, Multicolor', 100.0, 560)\n",
      "\n",
      "Final Recommendation:\n",
      "1 - Produto: Domez My Hero Academia 4-Piece Bundle, Valor: 1\n",
      "2 - Produto: Heroes of Goo Jit Zu 41012 S1 Ultimate Hero Pk, Multicolor, Valor: 4\n",
      "3 - Produto: Avengers Marvel Black Panther 6\"-Scale Marvel Super Hero Action Figure Toy, Valor: 13\n",
      "4 - Produto: Avengers Marvel Endgame Titan Hero Power Fx Iron Man, Valor: 18\n",
      "5 - Produto: Marvel Spider-Man Titan Hero Series Villains Sandman Figure, Valor: 24\n",
      "6 - Produto: Avengers Marvel Endgame Titan Hero Series Black Panther 12\" Action Figure, Brown/A, Valor: 25\n",
      "7 - Produto: Avengers Marvel Titan Hero Series Black Panther Action Figure, Valor: 26\n",
      "8 - Produto: Disney's Big Hero 6: Baymax Ultra Detail Figure, Multicolor, Valor: 27\n",
      "9 - Produto: Funko POP! Animation: My Hero Academia - All Might Collectible Figure, Multicolor, Valor: 34\n",
      "10 - Produto: Forum Novelties Child Super Hero Gauntlet Gloves, Red, Valor: 40\n"
     ]
    }
   ],
   "source": [
    "## MAIN TO RUN CODE\n",
    "# pensar em forma de testar código no dataset\n",
    "dataset_1 = load_data()\n",
    "dataset_2 = load_data()\n",
    "print(\"Please select option:\")\n",
    "print(\"1- Select input from database\")\n",
    "print(\"2- Type product name\")\n",
    "user_input = int(input(\"Enter 1 or 2: \"))\n",
    "\n",
    "if user_input == 1:\n",
    "    product_line = int(input(\"Type the product's number: \"))\n",
    "    product_line = product_line - 2\n",
    "\n",
    "    if 0 <= product_line < len(dataset_1):\n",
    "        selected_product = dataset_1.iloc[product_line]  # Selects the product by its line in the database\n",
    "        print(\"\\nSelected Product:\")\n",
    "        print(selected_product['Product Name'])\n",
    "    else:\n",
    "        print(\"Invalid line Number.\")\n",
    "    \n",
    "    product_name = selected_product['Product Name']\n",
    "    category_filtered = category_filter(dataset_1, selected_product)\n",
    "    # print(category_filter)\n",
    "\n",
    "    # Word2Vec Filter\n",
    "    name_based_filter = name_based_filter(category_filtered, product_name)\n",
    "    print(\"\\n\")\n",
    "    print(\"Word2Vec with Category Filter: \")\n",
    "    print(name_based_filter['Product Name'][:5])\n",
    "    list_word2vec = name_based_filter['Product Name'].tolist()\n",
    "\n",
    "    # RapidFuzz Filter\n",
    "    print(\"\\n\")\n",
    "    print(\"RapidFuzz with no Category Filter: \")\n",
    "    for i in range(5):\n",
    "        print(prototype_rapid_fuzz_filter(user_input=product_name, products=dataset_2[\"Product Name\"], number_of_rec=5)[i])\n",
    "    list_rapidfuzz = (prototype_rapid_fuzz_filter(user_input=product_name, products=dataset_2[\"Product Name\"], number_of_rec=dataset_2.shape[0]))\n",
    "    \n",
    "elif user_input == 2:\n",
    "    product_name = input(\"Type the product's name: \")\n",
    "\n",
    "    # Word2Vec Filter\n",
    "    name_based_filter = name_based_filter(dataset_1, product_name)\n",
    "    print(\"Word2Vec: \")\n",
    "    print(name_based_filter['Product Name'][:5])\n",
    "    list_word2vec = name_based_filter['Product Name'].tolist()\n",
    "\n",
    "    # RapidFuzz Filter\n",
    "    print(\"\\n\")\n",
    "    print(\"RapidFuzz: \")\n",
    "    for i in range(5):\n",
    "        print(prototype_rapid_fuzz_filter(user_input=product_name, products=dataset_2[\"Product Name\"], number_of_rec=5)[i])\n",
    "    list_rapidfuzz = (prototype_rapid_fuzz_filter(user_input=product_name, products=dataset_2[\"Product Name\"], number_of_rec=dataset_2.shape[0]))\n",
    "\n",
    "else:\n",
    "    print(\"Not a valid number\")\n",
    "\n",
    "final_list = []\n",
    "for i in range(len(list_word2vec)):\n",
    "    for j in range(len(list_rapidfuzz)):\n",
    "        if (list_word2vec[i]==list_rapidfuzz[j][0]):\n",
    "            final_list.append([list_word2vec[i], i + j])\n",
    "        \n",
    "final_list.sort(key=lambda x: x[1])\n",
    "print(\"\\nFinal Recommendation:\")\n",
    "for idx, item in enumerate(final_list[:10], start=1):\n",
    "    print(f\"{idx} - Produto: {item[0]}, Valor: {item[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisitos #04 (Reference Recommendation Algorithm - Medium)\n",
    "def load_reference_database():\n",
    "    customers = pd.read_csv('data/recommend_1.csv') \n",
    "    transactions = pd.read_csv('data/trx_data.csv')\n",
    "    return customers,transactions\n",
    "\n",
    "def data_reference_manipulation(customers,transactions):\n",
    "    transactions['products'] = transactions['products'].apply(lambda x: [int(i) for i in x.split('|')])\n",
    "    data = pd.melt(transactions.set_index('customerId')['products'].apply(pd.Series).reset_index(), \n",
    "                id_vars=['customerId'],\n",
    "                value_name='products') \\\n",
    "        .dropna().drop(['variable'], axis=1) \\\n",
    "        .groupby(['customerId', 'products']) \\\n",
    "        .agg({'products': 'count'}) \\\n",
    "        .rename(columns={'products': 'purchase_count'}) \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'products': 'productId'})\n",
    "    data['productId'] = data['productId'].astype(np.int64)\n",
    "    return data\n",
    "\n",
    "def create_data_dummy(data):\n",
    "    data_dummy = data.copy()\n",
    "    data_dummy['purchase_dummy'] = 1\n",
    "    return data_dummy\n",
    "\n",
    "def data_model_input():\n",
    "    customers,transactions = load_reference_database()\n",
    "    data = data_reference_manipulation(customers,transactions)\n",
    "    data_dummy = create_data_dummy(data)\n",
    "    df_matrix = pd.pivot_table(data, values='purchase_count', index='customerId', columns='productId')\n",
    "    df_matrix_norm = (df_matrix-df_matrix.min())/(df_matrix.max()-df_matrix.min())\n",
    "\n",
    "    # create a table for input to the modeling  \n",
    "    d = df_matrix_norm.reset_index() \n",
    "    d.index.names = ['scaled_purchase_freq'] \n",
    "    data_norm = pd.melt(d, id_vars=['customerId'], value_name='scaled_purchase_freq').dropna()\n",
    "\n",
    "    return data,data_dummy,data_norm\n",
    "#function that combines steps above\n",
    "def normalize_data(data):\n",
    "    df_matrix = pd.pivot_table(data, values='purchase_count', index='customerId', columns='productId')\n",
    "    df_matrix_norm = (df_matrix-df_matrix.min())/(df_matrix.max()-df_matrix.min())\n",
    "    d = df_matrix_norm.reset_index()\n",
    "    d.index.names = ['scaled_purchase_freq']\n",
    "    return pd.melt(d, id_vars=['customerId'], value_name='scaled_purchase_freq').dropna()\n",
    "\n",
    "def split_data(data):\n",
    "    '''\n",
    "    Splits dataset into training and test set.\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame)\n",
    "        \n",
    "    Returns\n",
    "        train_data (tc.SFrame)\n",
    "        test_data (tc.SFrame)\n",
    "    '''\n",
    "    train, test = train_test_split(data, test_size = .2)\n",
    "    train_data = tc.SFrame(train)\n",
    "    test_data = tc.SFrame(test)\n",
    "    return train_data, test_data\n",
    "\n",
    "def model(train_data, name, user_id, item_id, target, users_to_recommend, n_rec, n_display):\n",
    "    if name == 'popularity':\n",
    "        model = tc.popularity_recommender.create(train_data, \n",
    "                                                    user_id=user_id, \n",
    "                                                    item_id=item_id, \n",
    "                                                    target=target)\n",
    "    elif name == 'cosine':\n",
    "        model = tc.item_similarity_recommender.create(train_data, \n",
    "                                                    user_id=user_id, \n",
    "                                                    item_id=item_id, \n",
    "                                                    target=target, \n",
    "                                                    similarity_type='cosine')\n",
    "    elif name == 'pearson':\n",
    "        model = tc.item_similarity_recommender.create(train_data, \n",
    "                                                    user_id=user_id, \n",
    "                                                    item_id=item_id, \n",
    "                                                    target=target, \n",
    "                                                    similarity_type='pearson')\n",
    "        \n",
    "    recom = model.recommend(users=users_to_recommend, k=n_rec)\n",
    "    recom.print_rows(n_display)\n",
    "    return model\n",
    "\n",
    "# To Run \n",
    "\"\"\"\n",
    "data,data_dummy,data_norm = data_model_input()\n",
    "train_data, test_data = split_data(data)\n",
    "train_data_dummy, test_data_dummy = split_data(data_dummy)\n",
    "train_data_norm, test_data_norm = split_data(data_norm)\n",
    "\n",
    "# constant variables to define field names include:\n",
    "name = 'cosine'\n",
    "user_id = 'customerId'\n",
    "item_id = 'productId'\n",
    "target = 'purchase_count'\n",
    "users_to_recommend = list(customers[user_id])\n",
    "n_rec = 10 # number of items to recommend\n",
    "n_display = 30 # to display the first few rows in an output dataset\n",
    "\n",
    "cos = model(train_data, name, user_id, item_id, target, users_to_recommend, n_rec, n_display)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisitos #05 (Comparison between our recommendation and the reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisitos #06 (Recomendation Algorithm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
