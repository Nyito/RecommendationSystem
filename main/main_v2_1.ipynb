{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/leo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/leo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rapidfuzz import fuzz, process, utils\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') \n",
    "\n",
    "# Importing dataset\n",
    "dataset_1 = pd.read_csv(\"../data/AmazonData.csv\")\n",
    "dataset_2 = pd.read_csv(\"../data/AmazonData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset process function\n",
    "def dataset_process(dataset):\n",
    "\n",
    "    # Excluding columns that we dont use\n",
    "    cols = [0,2,3,5,6,8,9,12,14,15,16,17,18,19,20,21,22,23,24,25,26,27]\n",
    "    dataset.drop(dataset.columns[cols], axis =1, inplace=True)\n",
    "    dataset.dropna(inplace = True)\n",
    "\n",
    "    # Splitting Category in 3 parts\n",
    "    new = dataset[\"Category\"].str.split(\"|\", n = 3, expand = True)\n",
    "    \n",
    "    # making the first category called Main Category\n",
    "    dataset[\"Main Category\"]= new[0] \n",
    "    \n",
    "    # making the second category called sub_category \n",
    "    dataset[\"Sub Category\"]= new[1]\n",
    "\n",
    "    # making the third category called side_category \n",
    "    dataset[\"Side Category\"]= new[2]\n",
    "\n",
    "    # making the last column consist of the remaining categories\n",
    "    dataset[\"Other Category\"]= new[3]\n",
    "\n",
    "    # Dropping old category columns and the remaining categories \n",
    "    dataset.drop(columns =[\"Category\"], inplace = True)\n",
    "\n",
    "    # Setting Column Selling Price as float value\n",
    "    # Database Price and weight treatment\n",
    "    dataset.rename(columns = {'Uniq Id':'Id','Shipping Weight':'Shipping Weight(Pounds)', 'Selling Price':'Selling Price($)'}, inplace = True)\n",
    "\n",
    "    # Removing units from Price and Weight\n",
    "    dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].str.strip('ounces')\n",
    "    dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].str.strip('pounds')\n",
    "    dataset['Selling Price($)'] = dataset['Selling Price($)'].str.replace('$', '')\n",
    "\n",
    "    # Removing rows with Total Price invalid\n",
    "    indexes = dataset[dataset['Selling Price($)'] == 'Total price:'].index\n",
    "    dataset.drop(indexes, inplace=True)\n",
    "\n",
    "    # Removing rows with '-' character\n",
    "    dataset['Selling Price($)'] = dataset['Selling Price($)'].str.replace(',', '', regex=False)\n",
    "    indexes = dataset[dataset['Selling Price($)'].str.contains('-', na=False)].index\n",
    "    dataset.drop(indexes, inplace=True)\n",
    "\n",
    "    # Removing rows with '&' character\n",
    "    indexes = dataset[dataset['Selling Price($)'].str.contains('&', na=False)].index\n",
    "    dataset.drop(indexes, inplace=True)\n",
    "\n",
    "    # Removing rows with 'Currently' character\n",
    "    indexes = dataset[dataset['Selling Price($)'].str.contains('Currently', na=False)].index\n",
    "    dataset.drop(indexes, inplace=True)\n",
    "\n",
    "    # Removing rows with 'from' character\n",
    "    indexes = dataset[dataset['Selling Price($)'].str.contains('from', na=False)].index\n",
    "    dataset.drop(indexes, inplace=True)\n",
    "\n",
    "    # Adjusting values with wrong format\n",
    "    dataset['Selling Price($)'] = dataset['Selling Price($)'].str.split(' ').str[0]\n",
    "    dataset['Selling Price($)'] = dataset['Selling Price($)'].astype(float)\n",
    "\n",
    "    # Setting Column Shipping Weight as float value\n",
    "    indexes = dataset[dataset['Shipping Weight(Pounds)'].str.contains(r'\\. ', na=False)].index\n",
    "\n",
    "    dataset.at[1619, 'Shipping Weight(Pounds)']\n",
    "    dataset.drop(1619, inplace=True)\n",
    "    dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].str.replace(',', '', regex=False)\n",
    "    dataset['Shipping Weight(Pounds)'] = dataset['Shipping Weight(Pounds)'].astype(float)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing text function\n",
    "def preprocess_text(text):\n",
    "    text = text.replace('[^a-zA-Z]',' ').lower()\n",
    "    stop_re = '\\\\b'+'\\\\b|\\\\b'.join(nltk.corpus.stopwords.words('english'))+'\\\\b'\n",
    "    text = text.replace(stop_re, '')\n",
    "    text = text.split()\n",
    "\n",
    "    # Add lemmatization using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Word2Vec model with a vector size of 100, using the 'Processed Product Name' column.\n",
    "def train_word2vec_model(dataset):\n",
    "    model = Word2Vec(sentences=dataset[\"Processed Product Name\"], vector_size=100, window=50, min_count=1, workers=4)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing text function\n",
    "def vectorize_product(product_name, model):\n",
    "\n",
    "    words = [word for word in product_name if word in model.wv]\n",
    "    if len(words) > 0:\n",
    "        return np.mean([model.wv[word] for word in words], axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.wv.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prototype_rapid_fuzz_filter(user_input, products, number_of_rec):\n",
    "    list_of_rec = []\n",
    "    token_set_ratio_match = process.extract(user_input, products, scorer=fuzz.token_set_ratio, limit=1, processor=utils.default_process)\n",
    "    partial_ratio_matches = process.extract(user_input, products, scorer=fuzz.token_set_ratio, limit=number_of_rec, processor=utils.default_process)\n",
    "\n",
    "    if token_set_ratio_match[0][1] == 100:\n",
    "        list_of_rec.extend(token_set_ratio_match)\n",
    "        for match in partial_ratio_matches:\n",
    "            if match[0] != token_set_ratio_match[0][0]:\n",
    "                list_of_rec.append(match)\n",
    "    \n",
    "    else:\n",
    "        for match in partial_ratio_matches:\n",
    "            list_of_rec.append(match)\n",
    "\n",
    "    return list_of_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Recommendation function\n",
    "def product_recommendation(product_vector, dataset, top_n=5):\n",
    "    \n",
    "    # Calcular similaridades cosseno\n",
    "    similarities = dataset[\"Product Vector\"].apply(lambda x: cosine_similarity([product_vector], [x])[0][0])\n",
    "    \n",
    "    # Ordenar por similaridade e pegar os top_n produtos\n",
    "    top_indices = similarities.nlargest(top_n).index\n",
    "    \n",
    "    # Retornar o DataFrame com os produtos recomendados, mas mantendo os nomes originais\n",
    "    return dataset.loc[top_indices, dataset.columns != 'Product Vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category Filter \n",
    "def category_filter(dataset, selected_product):\n",
    "\n",
    "    main_category_input = selected_product['Main Category']\n",
    "    sub_category_input = selected_product['Sub Category']\n",
    "    side_category_input = selected_product['Side Category']\n",
    "    other_category_input = selected_product['Other Category']\n",
    "\n",
    "    # Create a new column to calculate the score\n",
    "    dataset['score'] = 0\n",
    "\n",
    "    # Raises the score if categories match\n",
    "    dataset.loc[dataset['Main Category'] == main_category_input, 'score'] += 1\n",
    "    dataset.loc[dataset['Sub Category'] == sub_category_input, 'score'] += 1\n",
    "    dataset.loc[dataset['Side Category'] == side_category_input, 'score'] += 1\n",
    "    dataset.loc[dataset['Other Category'] == other_category_input, 'score'] += 1\n",
    "\n",
    "    # Sort the database based on the score\n",
    "    category_filter = dataset.sort_values(by='score', ascending=False)\n",
    "\n",
    "    max_score = category_filter['score'].max()\n",
    "\n",
    "    # Filter rows with the maximum score\n",
    "    category_filter = category_filter[category_filter['score'] == max_score]\n",
    "\n",
    "    # Removes the new column\n",
    "    category_filter = category_filter.drop(columns='score')\n",
    "\n",
    "    # return the sorted database\n",
    "    if category_filter.empty:\n",
    "        print('No recommendaation found for this product.')\n",
    "    else:\n",
    "        return category_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def name_based_filter(dataset, product_name):\n",
    "\n",
    "    # Applying text preprocess in dataset\n",
    "    dataset[\"Processed Product Name\"] = dataset[\"Product Name\"].apply(preprocess_text)\n",
    "\n",
    "    model = train_word2vec_model(dataset)\n",
    "\n",
    "    # Applying vectorizing function in dataset\n",
    "    dataset[\"Product Vector\"] = dataset[\"Processed Product Name\"].apply(lambda x: vectorize_product(x, model))\n",
    "\n",
    "    # Pré-processando o nome do produto fornecido pelo usuário\n",
    "    processed_product_name = preprocess_text(product_name)\n",
    "\n",
    "    # Vetorizando o nome do produto fornecido (passar o modelo aqui também)\n",
    "    product_vector = vectorize_product(processed_product_name, model)\n",
    "\n",
    "    recommendation = product_recommendation(product_vector, dataset, dataset.shape[0])\n",
    "    return recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/envs/tcc/lib/python3.7/site-packages/ipykernel_launcher.py:34: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n"
     ]
    }
   ],
   "source": [
    "dataset_1 = dataset_process(dataset_1)\n",
    "dataset_2 = dataset_process(dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select option:\n",
      "1- Select input from database\n",
      "2- Type product name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Product:\n",
      "Aurora World 15919 8\" Dr. Seuss Fish Playset, 8\", Red, Blue, Yellow, Green\n",
      "\n",
      "\n",
      "Word2Vec with Category Filter: \n",
      "6465    Aurora World 15919 8\" Dr. Seuss Fish Playset, ...\n",
      "6488    Twisty Petz Cuddlez Purrella Kitty Transformin...\n",
      "2624    Aurora World YooHoo Plush Toy Animal, Camel Ch...\n",
      "1501        MerryMakers Pout-Pout Fish Plush Doll, 9-Inch\n",
      "1670                              Delta Plush Toy w Sound\n",
      "Name: Product Name, dtype: object\n",
      "\n",
      "\n",
      "RapidFuzz with no Category Filter: \n",
      "('Aurora World 15919 8\" Dr. Seuss Fish Playset, 8\", Red, Blue, Yellow, Green', 100.0, 6465)\n",
      "('Aurora World Aurora Dr. Seuss Cindy Lou Who 12\", Grinch 18\", and his Dog Max 4\" Clip On Special Set of 3 Plush Toys, Green, Red, Brown, Yellow', 74.50980392156863, 3972)\n",
      "('Aurora World Inc., Lynx', 72.72727272727272, 3136)\n",
      "('Aurora World 8\" Goat Kid Toy', 68.29268292682927, 2554)\n",
      "('Aurora World 8\" Sting Ray Toy', 66.66666666666666, 823)\n",
      "\n",
      "Final Recommendation:\n",
      "1 - Produto: Aurora World 15919 8\" Dr. Seuss Fish Playset, 8\", Red, Blue, Yellow, Green, Valor: 0\n",
      "2 - Produto: Aurora World Sea Sparkles Mermaid, Bride, Valor: 60\n",
      "3 - Produto: Aurora World YooHoo Plush Toy Animal, Camel Chameleon, Valor: 90\n",
      "4 - Produto: Aurora World Sea Sparkles Amethyst Mermaid Plush, Valor: 126\n",
      "5 - Produto: Linzy Plush 16\" Yellow Aissa Doll Soft Rag Doll, Valor: 136\n",
      "6 - Produto: Rhode Island Novelty 20\" Dino Bp T-rex , Green, Valor: 261\n",
      "7 - Produto: Crayola 12\" Deluxe Color ’N Plush Puppy - Draw, Wash Reuse, Valor: 293\n",
      "8 - Produto: Fiesta Toys Lubby Cubbies Blue Berry The Penguin Plush Stuffed Toy - 3.5 Inches, Valor: 392\n",
      "9 - Produto: Comic Images Super Deformed Plush Classic '90S Nick Toons Ren Plush Figure, Valor: 521\n",
      "10 - Produto: Great Eastern Entertainment Dragon Ball Super-SS Vegeta 01 6.5\" Plush, Valor: 637\n"
     ]
    }
   ],
   "source": [
    "print(\"Please select option:\")\n",
    "print(\"1- Select input from database\")\n",
    "print(\"2- Type product name\")\n",
    "user_input = int(input(\"Enter 1 or 2: \"))\n",
    "\n",
    "main_category_input = None\n",
    "sub_category_input = None\n",
    "side_category_input = None\n",
    "other_category_input = None\n",
    "\n",
    "list_word2vec = []\n",
    "\n",
    "if user_input == 1:\n",
    "    product_line = int(input(\"Type the product's number: \"))\n",
    "    product_line = product_line - 2\n",
    "\n",
    "    if 0 <= product_line < len(dataset_1):\n",
    "        selected_product = dataset_1.iloc[product_line]  # Selects the product by its line in the database\n",
    "        print(\"\\nSelected Product:\")\n",
    "        print(selected_product['Product Name'])\n",
    "    else:\n",
    "        print(\"Invalid line Number.\")\n",
    "    \n",
    "    product_name = selected_product['Product Name']\n",
    "    category_filter = category_filter(dataset_1, selected_product)\n",
    "    # print(category_filter)\n",
    "\n",
    "    # Word2Vec Filter\n",
    "    name_based_filter = name_based_filter(category_filter, product_name)\n",
    "    print(\"\\n\")\n",
    "    print(\"Word2Vec with Category Filter: \")\n",
    "    print(name_based_filter['Product Name'][:5])\n",
    "    list_word2vec = name_based_filter['Product Name'].tolist()\n",
    "\n",
    "    # RapidFuzz Filter\n",
    "    print(\"\\n\")\n",
    "    print(\"RapidFuzz with no Category Filter: \")\n",
    "    for i in range(5):\n",
    "        print(prototype_rapid_fuzz_filter(user_input=product_name, products=dataset_2[\"Product Name\"], number_of_rec=5)[i])\n",
    "    list_rapidfuzz = (prototype_rapid_fuzz_filter(user_input=product_name, products=dataset_2[\"Product Name\"], number_of_rec=dataset_2.shape[0]))\n",
    "    \n",
    "elif user_input == 2:\n",
    "    product_name = input(\"Type the product's name: \")\n",
    "\n",
    "    # Word2Vec Filter\n",
    "    name_based_filter = name_based_filter(dataset_1, product_name)\n",
    "    print(\"Word2Vec: \")\n",
    "    print(name_based_filter['Product Name'][:5])\n",
    "    list_word2vec = name_based_filter['Product Name'].tolist()\n",
    "\n",
    "    # RapidFuzz Filter\n",
    "    print(\"\\n\")\n",
    "    print(\"RapidFuzz: \")\n",
    "    for i in range(5):\n",
    "        print(prototype_rapid_fuzz_filter(user_input=product_name, products=dataset_2[\"Product Name\"], number_of_rec=5)[i])\n",
    "    list_rapidfuzz = (prototype_rapid_fuzz_filter(user_input=product_name, products=dataset_2[\"Product Name\"], number_of_rec=dataset_2.shape[0]))\n",
    "\n",
    "else:\n",
    "    print(\"Not a valid number\")\n",
    "\n",
    "final_list = []\n",
    "for i in range(len(list_word2vec)):\n",
    "    for j in range(len(list_rapidfuzz)):\n",
    "        if (list_word2vec[i]==list_rapidfuzz[j][0]):\n",
    "            final_list.append([list_word2vec[i], i + j])\n",
    "        \n",
    "final_list.sort(key=lambda x: x[1])\n",
    "print(\"\\nFinal Recommendation:\")\n",
    "for idx, item in enumerate(final_list[:10], start=1):\n",
    "    print(f\"{idx} - Produto: {item[0]}, Valor: {item[1]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
